{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "import api_keys"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_openai import ChatOpenAI\n",
    "from typing import TypedDict, Annotated\n",
    "from pydantic import BaseModel, Field"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_core.messages import SystemMessage\n",
    "\n",
    "class Profile(BaseModel):\n",
    "    character_name: str\n",
    "    universe: str\n",
    "    requirements: str\n",
    "    user_name: str\n",
    "\n",
    "profile_system_prompt = '''Your role is to become a character who engages in conversation with the user.\n",
    "To do this, you should collect the following information from the user:\n",
    "\n",
    "- What the character's name is\n",
    "- What universe(세계관, 영화, 게임 등) does the character belong to\n",
    "- What the user's requirements are\n",
    "- Whtr the user's name is\n",
    "\n",
    "If you cannot determine this information, ask the user directly to clarify — preferably using a bullet-point or structured format. Do not make assumptions.\n",
    "Once you have all the necessary information, confirm it with the user one more time, and then call the relevant tool.'''\n",
    "\n",
    "\n",
    "\n",
    "llm  = ChatOpenAI(model=\"gpt-4o-mini\")\n",
    "profiling_llm = llm.bind_tools([Profile])\n",
    "\n",
    "# profiling_llm.invoke([(\"assistant\", profile_system_prompt),(\"user\", \"character name:지우, universe: 포켓몬스터, requirements: 없음, my name is 여행자. 정보 확인완료. 바로 시작하자\")])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 웹 검색 툴"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_community.tools.tavily_search import TavilySearchResults\n",
    "\n",
    "web_search_tool = TavilySearchResults(max_results=5)\n",
    "# web_search_tool.invoke({\"query\": \"RAG란?\"})"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Profile 검색 쿼리"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_core.output_parsers import StrOutputParser\n",
    "from langchain_core.prompts import ChatPromptTemplate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "profile_search_system_prompt = \"\"\"Your role is to investigate character information based on the details provided by the user.\n",
    "Given a character's name and universe, generate the most effective and natural web search query to gather information about the character's background, personality, and dialogues.\n",
    "\n",
    "You MUST output the search query only.\"\"\"\n",
    "\n",
    "profile_web_search_prompt = ChatPromptTemplate.from_messages(\n",
    "    [\n",
    "        (\"system\", profile_search_system_prompt),\n",
    "        (\"user\", \"Character profile:\\n{profile}\")\n",
    "    ]\n",
    ")\n",
    "\n",
    "profile_web_search_chain = profile_web_search_prompt | llm | StrOutputParser()\n",
    "# web_query = profile_web_search_chain.invoke({\"profile\": {\"name\": \"지우\", \"universe\": \"포켓몬스터\", \"requirements\": \"없음\"}})\n",
    "# print(web_query)\n",
    "# web_search_tool.invoke(web_query)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Vector DB 및 Retriever 선언"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.text_splitter import RecursiveCharacterTextSplitter\n",
    "from langchain_community.vectorstores import Chroma\n",
    "from langchain_openai import OpenAIEmbeddings\n",
    "from langchain_core.documents import Document"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[Document(metadata={}, page_content='RAG 아키텍처를 사용하면 조직은 모델을 미세 조정하거나 사전 학습시키는 데 비용과 시간을 들이지 않고 소량의 데이터를 제공하여 LLM 모델을 배포하고 이를 보강하여 조직에 관련성 높은 결과를 반환할 수 있습니다.\\n\\nRAG의 사용 사례에는 어떤 것이 있나요?\\n\\nRAG에는 다양한 사용 사례가 있습니다. 가장 일반적인 사용 사례는 다음과 같습니다.\\n\\nRAG의 이점은 무엇인가요?\\n\\nRAG 접근 방식에는 다음과 같은 여러 가지 주요 이점이 있습니다. [...] 어떤 경우에 RAG를 사용하고 모델을 미세 조정해야 하나요?'),\n",
       " Document(metadata={}, page_content='RAG 아키텍처를 사용하면 조직은 모델을 미세 조정하거나 사전 학습시키는 데 비용과 시간을 들이지 않고 소량의 데이터를 제공하여 LLM 모델을 배포하고 이를 보강하여 조직에 관련성 높은 결과를 반환할 수 있습니다.\\n\\nRAG의 사용 사례에는 어떤 것이 있나요?\\n\\nRAG에는 다양한 사용 사례가 있습니다. 가장 일반적인 사용 사례는 다음과 같습니다.\\n\\nRAG의 이점은 무엇인가요?\\n\\nRAG 접근 방식에는 다음과 같은 여러 가지 주요 이점이 있습니다. [...] 어떤 경우에 RAG를 사용하고 모델을 미세 조정해야 하나요?'),\n",
       " Document(metadata={}, page_content='RAG 기술은 질의응답, 정보 검색, 팩트 체킹 등의 태스크에서 활발히 연구되고 있으며, 구글의 LaMDA, OpenAI의 WebGPT 등 최신 LLM에도 적용되고 있습니다. 다만 RAG 모델의 성능은 연결된 지식 베이스의 품질과 커버리지에 크게 의존하므로, 고품질의 지식 베이스 구축이 중요한 과제로 남아 있습니다.'),\n",
       " Document(metadata={}, page_content='RAG 기술은 질의응답, 정보 검색, 팩트 체킹 등의 태스크에서 활발히 연구되고 있으며, 구글의 LaMDA, OpenAI의 WebGPT 등 최신 LLM에도 적용되고 있습니다. 다만 RAG 모델의 성능은 연결된 지식 베이스의 품질과 커버리지에 크게 의존하므로, 고품질의 지식 베이스 구축이 중요한 과제로 남아 있습니다.'),\n",
       " Document(metadata={}, page_content=\"RAG는 유용한 출발점으로, 쉽게 사용할 수 있으며 일부 사용 사례에 사용하기에 충분합니다. 미세 조정은 LLM의 동작을 변경하거나 다른 '언어'의 학습과 같은 다양한 상황에서 가장 적합합니다. RAG와 미세 조정은 상호 배타적이지 않습니다. 이후 단계에서 도메인 언어와 원하는 출력 형식을 더 잘 이해하기 위해 모델을 미세 조정하는 것을 고려하고 RAG를 사용하여 응답의 품질과 관련성을 향상시키는 것도 가능합니다.\")]"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\n",
    "\n",
    "embd = OpenAIEmbeddings()\n",
    "\n",
    "vectorstore = Chroma(\n",
    "    collection_name=\"rag-chroma\",\n",
    "    embedding_function=embd\n",
    ")\n",
    "\n",
    "doc_retriever = vectorstore.as_retriever(search_kwargs={\"k\": 5})\n",
    "\n",
    "# search_result = web_search_tool.invoke({\"query\": \"RAG란?\"})\n",
    "\n",
    "# docs = [Document(page_content=result[\"content\"]) for result in search_result]\n",
    "\n",
    "# text_splitter = RecursiveCharacterTextSplitter.from_tiktoken_encoder(\n",
    "#     chunk_size=600, chunk_overlap=100\n",
    "# )\n",
    "\n",
    "# doc_splits = text_splitter.split_documents(docs)\n",
    "\n",
    "# vectorstore.add_documents(doc_splits)\n",
    "\n",
    "# doc_retriever.invoke(\"RAG에서 학습이 진행되나요?\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Retrieval 평가 Chain 정의"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "class EvalDocuments(BaseModel):\n",
    "    \"\"\"\n",
    "    Document evaluation class.\n",
    "    The decision attribute can have a value of 'yes' or 'no' indicating the relevance of the document to the message.\n",
    "    \"\"\"\n",
    "    decision: str = Field(description=\"Documents are relevant to message. This attribute can have a value of 'yes' or 'no'\")\n",
    "    \n",
    "\n",
    "llm = ChatOpenAI(model=\"gpt-4o-mini\", temperature=0)\n",
    "retrieval_llm_evaluator = llm.with_structured_output(EvalDocuments)\n",
    "\n",
    "retrieval_eval_system = \"\"\"You are an evaluator responsible for assessing whether a retrieved document is relevant to the user's message.\n",
    "The user is having a conversation with a character. If the document contains related keywords or is semantically connected to the user's message, evaluate it as relevant.\n",
    "Your goal is to filter out incorrectly retrieved documents.\n",
    "If the user's message is related to the document, output 'yes'; otherwise, output 'no'.\"\"\"\n",
    "\n",
    "retrieval_eval_prompt = ChatPromptTemplate.from_messages(\n",
    "    [\n",
    "        (\"system\", retrieval_eval_system),\n",
    "        (\"user\", \"Character Profile and User Name : {profile}\\n\\nRetrieved Document: {document}\\n\\nMessage: {message}\")\n",
    "    ]\n",
    ")\n",
    "\n",
    "retrieval_evaluator = retrieval_eval_prompt | retrieval_llm_evaluator\n",
    "\n",
    "# character_name = \"지우\"\n",
    "# universe = \"포켓몬스터\"\n",
    "# doc_text = \"지우의 파트너는 피카츄이다.\"\n",
    "# message = \"너의 파트너는 피카츄야.\"\n",
    "\n",
    "# print(retrieval_evaluator.invoke({\"profile\":{\"character_name\":character_name, \"universe\":universe}, \"document\":doc_text, \"message\":message}))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 필요정보 웹 검색 쿼리 작성 chain 정의"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "web_query_gen_system_prompt = \"\"\"You are role-playing with user and acting as a given character. To respond to the user's messages, you need to perform web searches.\n",
    "Your role is to generate an appropriate web search query to obtain the knowledge necessary to answer the user's messages.\n",
    "Output the web search query — do not output anything else.\"\"\"\n",
    "\n",
    "web_query_gen_prompt = ChatPromptTemplate.from_messages(\n",
    "    [\n",
    "        (\"system\", web_query_gen_system_prompt),\n",
    "        (\"user\", \"Character Profile and User Name : {profile}\\n\\n  User's Message: {message}\")\n",
    "    ]\n",
    ")\n",
    "\n",
    "llm = ChatOpenAI(model=\"gpt-4o-mini\")\n",
    "web_search_query_chain = web_query_gen_prompt | llm | StrOutputParser()\n",
    "# query = web_search_query_chain.invoke({\"profile\": {\"character_name\": \"지우\", \"universe\": \"피카츄\"}, \"message\": \"처음 포켓몬을 받은 마을은 어디였지?\"})\n",
    "# web_search_tool.invoke(query)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 답변 작성 Chain 정의하기"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain import hub\n",
    "\n",
    "# rag_prompt = hub.pull(\"rlm/rag-prompt\")\n",
    "\n",
    "llm = ChatOpenAI(model=\"gpt-4o-mini\")\n",
    "\n",
    "rag_system_prompt = \"\"\"You are a character engaged in a roleplay with the user.\n",
    "Respond to the user's messages based on the pieces of retrieved context provided.\n",
    "If the context is not needed, you may answer without using it.\n",
    "If you're asked something you don't know, simply say you don't know.\n",
    "Pay close attention to the context of the conversation provided by the user, and respond in a way that stays true to the profile of the character you are roleplaying.\n",
    "Always follow up your response with an appropriate question to keep the conversation going.\"\"\"\n",
    "\n",
    "rag_user_prompt = \"\"\"Character Profile and User Name : {profile}\n",
    "\n",
    "Context: {context}\n",
    "\n",
    "Conversations: {messages}\n",
    "\n",
    "Answer:\"\"\"\n",
    "\n",
    "rag_prompt = ChatPromptTemplate.from_messages(\n",
    "    [\n",
    "        (\"system\", rag_system_prompt),\n",
    "        (\"user\", rag_user_prompt)\n",
    "    ]\n",
    ")\n",
    "\n",
    "rag_chatbot_chain = rag_prompt | llm | StrOutputParser()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Eval Answer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_core.messages import AIMessage, HumanMessage\n",
    "\n",
    "def EvalResponse(BaseModel):\n",
    "    \"\"\"\n",
    "    Response evaluation class.\n",
    "    The decision attribute can have a value of 'yes' or 'no' indicating the relevance of the response to the conversation.\n",
    "    \"\"\"\n",
    "    decision: str = Field(description=\"Response is relevant to message. This attribute can have a value of 'yes' or 'no'\")\n",
    "    \n",
    "    \n",
    "llm = ChatOpenAI(model=\"gpt-4o-mini\", temperature=0)\n",
    "response_llm_evaluator = llm.with_structured_output(EvalResponse)\n",
    "\n",
    "response_eval_system = \"\"\"You are a character engaged in a roleplay with the user.\n",
    "Your role is to evaluate whether a response is appropriate to the conversation based on the character's profile and context.\n",
    "Determine whether your response is appropriate to the conversation.\n",
    "If your response properly addresses the issue or question in the conversation, output 'yes'; otherwise, output 'no'.\"\"\"\n",
    "\n",
    "\n",
    "\n",
    "response_eval_user_prompt = \"\"\"Character Profile and User Name : {profile}\n",
    "Context: {context}\n",
    "\n",
    "Conversations: {messages}\n",
    "\n",
    "Your Response: {response}\n",
    "\n",
    "Decision:\"\"\"\n",
    "\n",
    "response_eval_prompt = ChatPromptTemplate(\n",
    "    [\n",
    "        (\"system\", response_eval_system),\n",
    "        (\"user\", response_eval_user_prompt)\n",
    "    ]\n",
    ")\n",
    "\n",
    "response_eval_chain = response_eval_prompt | response_llm_evaluator"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Rewrite Query"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "llm = ChatOpenAI(model=\"gpt-4o-mini\")\n",
    "\n",
    "# Prompt\n",
    "system = \"\"\"You are a question rewriter that refines input queries to enhance their effectiveness for vector store retrieval or web searching by capturing their underlying semantic intent.\"\"\"\n",
    "\n",
    "re_write_prompt = ChatPromptTemplate.from_messages(\n",
    "    [\n",
    "        (\"system\", system),\n",
    "        (\"user\",\"Here is the original question:\\n{question}\\n\\nPlease rewrite it to improve clarity and optimize it for retrieval.\"),\n",
    "    ]\n",
    ")\n",
    "\n",
    "question_rewriter = re_write_prompt | llm\n",
    "# question_rewriter.invoke({\"question\": \"RAG에 대해서 알려주세요\"})"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Define Nodes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "from typing import Annotated\n",
    "from langgraph.graph.message import add_messages\n",
    "from langgraph.graph import START, END\n",
    "\n",
    "class ConversationState(TypedDict):\n",
    "    messages: Annotated[list, add_messages]\n",
    "    documents: Annotated[list, add_messages]\n",
    "    web_query: Annotated[list, add_messages]\n",
    "    web_search_flag: bool\n",
    "    generation: str\n",
    "    retry_flag: bool\n",
    "    retries:int\n",
    "    profile: dict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_profile_messages(messages):\n",
    "    return [SystemMessage(content=profile_system_prompt)] + messages\n",
    "\n",
    "\n",
    "def profiling(state):\n",
    "    print(\"--- Profiling Node ---\")\n",
    "    messages = get_profile_messages(state[\"messages\"])\n",
    "    response = profiling_llm.invoke(messages)\n",
    "    return {\"messages\": [response]}\n",
    "\n",
    "\n",
    "def route_message(state):\n",
    "    print(\"--- Routing Node ---\")\n",
    "    profile = state.get(\"profile\")\n",
    "    \n",
    "    if profile: # Profile 수집 완료 상태, 대화 시작\n",
    "        print(\"-- Route Message To Retriever --\")\n",
    "        return \"retriever\"\n",
    "    else: # 정보 수집 노드로 라우팅\n",
    "        print(\"-- Route To Profiling --\")\n",
    "        return \"profiling\"\n",
    "\n",
    "\n",
    "def check_profiling(state):\n",
    "    print(\"--- Check Profiling Node ---\")\n",
    "    messages = state[\"messages\"]\n",
    "    \n",
    "    if isinstance(messages[-1], AIMessage) and messages[-1].tool_calls: # Tool Call 발생했을 경우 Profile 수집 완료, \"profile_web_search\"로 라우팅\n",
    "        print(\"-- Route Message To Profile Web Search --\")\n",
    "        return \"profile_web_search\"\n",
    "    else:\n",
    "        print(\"-- Profile Information Insufficient --\")\n",
    "        return \"insufficient\"\n",
    "        \n",
    "        \n",
    "    \n",
    "def profile_web_search(state):\n",
    "    print(\"--- Profile Web Search Node ---\")\n",
    "    messages = state[\"messages\"]\n",
    "    profile = messages[-1].tool_calls[0][\"args\"]\n",
    "    \n",
    "    # 쿼리 재작성 카운터 초기화\n",
    "    retries = 0\n",
    "    \n",
    "    # 프로필 웹 검색 시작\n",
    "    profile_search_query = profile_web_search_chain.invoke({\"profile\": profile})\n",
    "    saerch_results = web_search_tool.invoke(profile_search_query)\n",
    "    \n",
    "    # 수집 문서 Chunking\n",
    "    docs = [Document(page_content=result[\"content\"]) for result in saerch_results]\n",
    "    splitter = RecursiveCharacterTextSplitter.from_tiktoken_encoder(\n",
    "        chunk_size=550, chunk_overlap=50\n",
    "    )\n",
    "    doc_splits = splitter.split_documents(docs)\n",
    "    \n",
    "    # 벡터 DB에 저장\n",
    "    vectorstore.add_documents(doc_splits) \n",
    "    \n",
    "    # tool_calls 메시지 삭제\n",
    "    messages.pop()\n",
    "    messages.append(AIMessage(content=\"정보 수집 완료. 대화를 시작해주세요!\"))\n",
    "    \n",
    "    return {\"messages\": messages, \"profile\": profile, \"web_query\": profile_search_query, \"retries\": retries}\n",
    "\n",
    "\n",
    "def retriever(state):\n",
    "    print(\"--- Retriever Node ---\")\n",
    "    user_message = state[\"messages\"][-1]\n",
    "    \n",
    "    documents = state.get(\"documents\", [])\n",
    "    print(user_message.content)\n",
    "    retrieved_docs = doc_retriever.invoke(user_message.content)\n",
    "    documents.extend(retrieved_docs)\n",
    "    \n",
    "    state[\"documents\"] = documents\n",
    "    return state\n",
    "    \n",
    "\n",
    "def evaluate_documents(state):\n",
    "    print(\"--- Evaluate Document Node ---\")\n",
    "    user_message = state[\"messages\"][-1]\n",
    "    documents = state[\"documents\"]\n",
    "    profile = state[\"profile\"]\n",
    "    \n",
    "    filtered_docs = []\n",
    "    web_search_flag = False\n",
    "    \n",
    "    for doc in documents:\n",
    "        evaluation = retrieval_evaluator.invoke(\n",
    "            {\"profile\":profile, \"document\":doc.page_content, \"message\":user_message}\n",
    "        )\n",
    "        if evaluation.decision == \"yes\":\n",
    "            print(\"--- Document Is Relevant To Message---\")\n",
    "            filtered_docs.append(doc)\n",
    "        else:\n",
    "            print(\"--- Document Is Not Relevant To Message---\")\n",
    "            web_search_flag = True\n",
    "    \n",
    "    state[\"documents\"] = filtered_docs\n",
    "    state[\"web_search_flag\"] = web_search_flag\n",
    "    \n",
    "    return state\n",
    "\n",
    "\n",
    "def decide_generation(state):\n",
    "    print(\"--- Decide Generation ---\")\n",
    "    web_search_flag = state[\"web_search_flag\"]\n",
    "    \n",
    "    if web_search_flag:\n",
    "        print(\"--- Route To Web Search ---\")\n",
    "        return \"web_search\"\n",
    "    else:\n",
    "        print(\"--- Route To Generate ---\")\n",
    "        return \"generate\"\n",
    "\n",
    "\n",
    "def web_search(state):\n",
    "    print(\"--- Web Search Node ---\")\n",
    "    messages = state[\"messages\"]\n",
    "    user_message = messages[-1]\n",
    "    documents = state[\"documents\"]\n",
    "    profile = state[\"profile\"]\n",
    "    \n",
    "    web_search_query = web_search_query_chain.invoke({\"profile\": profile, \"message\": user_message})\n",
    "    search_results = web_search_tool.invoke(web_search_query)\n",
    "    \n",
    "    docs = [Document(page_content=result[\"content\"]) for result in search_results]\n",
    "    splitter = RecursiveCharacterTextSplitter.from_tiktoken_encoder(\n",
    "        chunk_size=550, chunk_overlap=50\n",
    "    )\n",
    "    doc_splits = splitter.split_documents(docs)\n",
    "    \n",
    "    vectorstore.add_documents(doc_splits)\n",
    "    documents.extend(doc_splits)\n",
    "    state[\"documents\"] = documents\n",
    "    \n",
    "    return state\n",
    "\n",
    "    \n",
    "def generate(state):\n",
    "    print(\"--- Generate ---\")\n",
    "    messages = state[\"messages\"]\n",
    "    profile = state[\"profile\"]\n",
    "    context = state[\"documents\"]\n",
    "    \n",
    "    generation = rag_chatbot_chain.invoke({\"profile\": profile, \"context\": context, \"messages\": messages})\n",
    "    state[\"generation\"] = generation\n",
    "    \n",
    "    return state\n",
    "\n",
    "def evaluate_generation(state):\n",
    "    print(\"--- Evaluate Generation ---\")\n",
    "    messages = state[\"messages\"]\n",
    "    generation = state[\"generation\"]\n",
    "    documents = state[\"documents\"]\n",
    "    profile = state[\"profile\"]\n",
    "    retries = state[\"retries\"]\n",
    "    \n",
    "    retry_flag = False\n",
    "    if retries >= 3: # max retry 도달한 경우 바로 답변(무한루프 방지)\n",
    "        print(\"-- Reached The Maximum Number Of Retries. --\")\n",
    "        messages.append(generation)\n",
    "        state[\"messages\"] = messages\n",
    "        state[\"retry_flag\"] = retry_flag\n",
    "        state[\"retries\"] = 0\n",
    "        return state\n",
    "    else: # max retry 도달하지 않은 경우 Response 평가\n",
    "        evaluation = response_eval_chain.invoke({\"profile\": profile, \"context\": documents, \"messages\": messages, \"response\": generation})\n",
    "        if evaluation.decision == \"yes\":\n",
    "            print(\"-- Response Addresses The Conversation --\")\n",
    "            messages.append(generation)\n",
    "            state[\"messages\"] = messages\n",
    "            state[\"retry_flag\"] = retry_flag\n",
    "            state[\"retries\"] = 0\n",
    "            return state\n",
    "        else:\n",
    "            print(\"-- Response Does Not Address The Conversation --\")\n",
    "            retry_flag = True\n",
    "            state[\"retry_flag\"] = retry_flag\n",
    "            state[\"retries\"] += 1\n",
    "            del state[\"generation\"]\n",
    "            return state\n",
    "        \n",
    "        \n",
    "def decide_reponse(state):\n",
    "    print(\"--- Decide To Response ---\")\n",
    "    retry_flag = state['retry_flag']\n",
    "    \n",
    "    if retry_flag:\n",
    "        return \"response\"\n",
    "    else:\n",
    "        return \"rewrite_query\"\n",
    "    \n",
    "def rewrite_query(state):\n",
    "    print(\"--- Rewrite User Question ---\")\n",
    "    messages = state[\"messages\"]\n",
    "    user_message = messages[-1]\n",
    "    \n",
    "    new_user_message = question_rewriter.invoke({\"question\": user_message})\n",
    "    messages[-1] = new_user_message\n",
    "    state[\"messages\"] = messages\n",
    "    \n",
    "    return state"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 그래프 정의"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langgraph.graph import StateGraph, START, END\n",
    "from langgraph.checkpoint.memory import MemorySaver\n",
    "\n",
    "PersonaGraph = StateGraph(ConversationState)\n",
    "\n",
    "PersonaGraph.add_node(profiling)\n",
    "PersonaGraph.add_node(profile_web_search)\n",
    "PersonaGraph.add_node(retriever)\n",
    "PersonaGraph.add_node(evaluate_documents)\n",
    "PersonaGraph.add_node(web_search)\n",
    "PersonaGraph.add_node(generate)\n",
    "PersonaGraph.add_node(evaluate_generation)\n",
    "PersonaGraph.add_node(rewrite_query)\n",
    "\n",
    "PersonaGraph.add_conditional_edges(\n",
    "    START,\n",
    "    route_message,\n",
    "    {\n",
    "        \"retriever\": \"retriever\",\n",
    "        \"profiling\": \"profiling\",\n",
    "    }\n",
    ")\n",
    "\n",
    "\n",
    "\n",
    "PersonaGraph.add_conditional_edges(\n",
    "    \"profiling\",\n",
    "    check_profiling,\n",
    "    {\n",
    "        \"profile_web_search\": \"profile_web_search\",\n",
    "        \"insufficient\": END\n",
    "    }\n",
    ")\n",
    "\n",
    "PersonaGraph.add_edge(\"profile_web_search\", END)\n",
    "\n",
    "PersonaGraph.add_edge(\"retriever\", \"evaluate_documents\")\n",
    "\n",
    "PersonaGraph.add_conditional_edges(\n",
    "    \"evaluate_documents\",\n",
    "    decide_generation,\n",
    "    {\n",
    "        \"generate\": \"generate\",\n",
    "        \"web_search\": \"web_search\",\n",
    "    }\n",
    ")\n",
    "\n",
    "\n",
    "PersonaGraph.add_edge(\"web_search\", \"generate\")\n",
    "PersonaGraph.add_edge(\"generate\", \"evaluate_generation\")\n",
    "\n",
    "PersonaGraph.add_conditional_edges(\n",
    "    \"evaluate_generation\",\n",
    "    decide_reponse,\n",
    "    {\n",
    "        \"response\": END,\n",
    "        \"rewrite_query\": \"rewrite_query\"\n",
    "    }\n",
    ")\n",
    "\n",
    "PersonaGraph.add_edge(\"rewrite_query\", \"retriever\")\n",
    "\n",
    "memory = MemorySaver()\n",
    "app = PersonaGraph.compile(checkpointer=memory)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "---\n",
      "config:\n",
      "  flowchart:\n",
      "    curve: linear\n",
      "---\n",
      "graph TD;\n",
      "\t__start__([<p>__start__</p>]):::first\n",
      "\tprofiling(profiling)\n",
      "\tprofile_web_search(profile_web_search)\n",
      "\tretriever(retriever)\n",
      "\tevaluate_documents(evaluate_documents)\n",
      "\tweb_search(web_search)\n",
      "\tgenerate(generate)\n",
      "\tevaluate_generation(evaluate_generation)\n",
      "\trewrite_query(rewrite_query)\n",
      "\t__end__([<p>__end__</p>]):::last\n",
      "\tgenerate --> evaluate_generation;\n",
      "\tprofile_web_search --> __end__;\n",
      "\tretriever --> evaluate_documents;\n",
      "\trewrite_query --> retriever;\n",
      "\tweb_search --> generate;\n",
      "\t__start__ -.-> retriever;\n",
      "\t__start__ -.-> profiling;\n",
      "\tprofiling -.-> profile_web_search;\n",
      "\tprofiling -. &nbsp;insufficient&nbsp; .-> __end__;\n",
      "\tevaluate_documents -.-> generate;\n",
      "\tevaluate_documents -.-> web_search;\n",
      "\tevaluate_generation -. &nbsp;response&nbsp; .-> __end__;\n",
      "\tevaluate_generation -.-> rewrite_query;\n",
      "\tclassDef default fill:#f2f0ff,line-height:1.2\n",
      "\tclassDef first fill-opacity:0\n",
      "\tclassDef last fill:#bfb6fc\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from IPython.display import Image, display\n",
    "print(app.get_graph(xray=True).draw_mermaid())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"./graph_diagram.png\" width=\"300\"/>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "User: 안녕\n",
      "--- Routing Node ---\n",
      "-- Route To Profiling --\n",
      "--- Profiling Node ---\n",
      "--- Check Profiling Node ---\n",
      "-- Profile Information Insufficient --\n",
      "Assistant: 안녕하세요! 어떤 캐릭터와 대화하고 싶으신가요? 알려주셔야 할 정보가 몇 가지 있어요:\n",
      "\n",
      "- 캐릭터의 이름\n",
      "- 그 캐릭터가 속한 세계관(영화, 게임 등)\n",
      "- 사용자의 요구사항\n",
      "- 사용자의 이름\n",
      "\n",
      "이 정보를 알려주시면 준비된 캐릭터와 함께 대화해 드릴게요!\n",
      "User: 지우, 포켓몬스터, 딱히 없음, 여행자\n",
      "--- Routing Node ---\n",
      "-- Route To Profiling --\n",
      "--- Profiling Node ---\n",
      "--- Check Profiling Node ---\n",
      "-- Profile Information Insufficient --\n",
      "Assistant: 감사합니다! 정보를 정리해볼게요:\n",
      "\n",
      "- 캐릭터 이름: 지우\n",
      "- 세계관: 포켓몬스터\n",
      "- 요구사항: 딱히 없음\n",
      "- 사용자 이름: 여행자\n",
      "\n",
      "이 정보가 맞나요? 확인해 주시면 다음 단계로 진행할게요!\n",
      "User: 시작하자\n",
      "--- Routing Node ---\n",
      "-- Route To Profiling --\n",
      "--- Profiling Node ---\n",
      "--- Check Profiling Node ---\n",
      "-- Route Message To Profile Web Search --\n",
      "Assistant: \n",
      "--- Profile Web Search Node ---\n",
      "Assistant: 정보 수집 완료. 대화를 시작해주세요!\n",
      "User: 너의 최애 포켓몬은?\n",
      "--- Routing Node ---\n",
      "-- Route Message To Retriever --\n",
      "--- Retriever Node ---\n",
      "너의 최애 포켓몬은?\n"
     ]
    },
    {
     "ename": "NotImplementedError",
     "evalue": "Unsupported message type: <class 'list'>\nFor troubleshooting, visit: https://python.langchain.com/docs/troubleshooting/errors/MESSAGE_COERCION_FAILURE ",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mNotImplementedError\u001b[39m                       Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[19]\u001b[39m\u001b[32m, line 8\u001b[39m\n\u001b[32m      6\u001b[39m     \u001b[38;5;28mprint\u001b[39m(\u001b[33m\"\u001b[39m\u001b[33mGoodbye!\u001b[39m\u001b[33m\"\u001b[39m)\n\u001b[32m      7\u001b[39m     \u001b[38;5;28;01mbreak\u001b[39;00m\n\u001b[32m----> \u001b[39m\u001b[32m8\u001b[39m \u001b[43m\u001b[49m\u001b[38;5;28;43;01mfor\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mevent\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;129;43;01min\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mapp\u001b[49m\u001b[43m.\u001b[49m\u001b[43mstream\u001b[49m\u001b[43m(\u001b[49m\u001b[43m{\u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mmessages\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43m[\u001b[49m\u001b[43mHumanMessage\u001b[49m\u001b[43m(\u001b[49m\u001b[43mcontent\u001b[49m\u001b[43m=\u001b[49m\u001b[43muser_input\u001b[49m\u001b[43m)\u001b[49m\u001b[43m]\u001b[49m\u001b[43m}\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mconfig\u001b[49m\u001b[43m)\u001b[49m\u001b[43m:\u001b[49m\n\u001b[32m      9\u001b[39m \u001b[43m    \u001b[49m\u001b[38;5;28;43;01mfor\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mvalue\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;129;43;01min\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mevent\u001b[49m\u001b[43m.\u001b[49m\u001b[43mvalues\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[43m:\u001b[49m\n\u001b[32m     10\u001b[39m \u001b[43m        \u001b[49m\u001b[38;5;28;43mprint\u001b[39;49m\u001b[43m(\u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mAssistant:\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mvalue\u001b[49m\u001b[43m[\u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mmessages\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m[\u001b[49m\u001b[43m-\u001b[49m\u001b[32;43m1\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m.\u001b[49m\u001b[43mcontent\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\AI\\VSCodeProjects\\Persona-ChatAgent\\.venv\\Lib\\site-packages\\langgraph\\pregel\\__init__.py:2330\u001b[39m, in \u001b[36mPregel.stream\u001b[39m\u001b[34m(self, input, config, stream_mode, output_keys, interrupt_before, interrupt_after, debug, subgraphs)\u001b[39m\n\u001b[32m   2324\u001b[39m     get_waiter = \u001b[38;5;28;01mNone\u001b[39;00m  \u001b[38;5;66;03m# type: ignore[assignment]\u001b[39;00m\n\u001b[32m   2325\u001b[39m \u001b[38;5;66;03m# Similarly to Bulk Synchronous Parallel / Pregel model\u001b[39;00m\n\u001b[32m   2326\u001b[39m \u001b[38;5;66;03m# computation proceeds in steps, while there are channel updates.\u001b[39;00m\n\u001b[32m   2327\u001b[39m \u001b[38;5;66;03m# Channel updates from step N are only visible in step N+1\u001b[39;00m\n\u001b[32m   2328\u001b[39m \u001b[38;5;66;03m# channels are guaranteed to be immutable for the duration of the step,\u001b[39;00m\n\u001b[32m   2329\u001b[39m \u001b[38;5;66;03m# with channel updates applied only at the transition between steps.\u001b[39;00m\n\u001b[32m-> \u001b[39m\u001b[32m2330\u001b[39m \u001b[38;5;28;01mwhile\u001b[39;00m \u001b[43mloop\u001b[49m\u001b[43m.\u001b[49m\u001b[43mtick\u001b[49m\u001b[43m(\u001b[49m\u001b[43minput_keys\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43minput_channels\u001b[49m\u001b[43m)\u001b[49m:\n\u001b[32m   2331\u001b[39m     \u001b[38;5;28;01mfor\u001b[39;00m _ \u001b[38;5;129;01min\u001b[39;00m runner.tick(\n\u001b[32m   2332\u001b[39m         loop.tasks.values(),\n\u001b[32m   2333\u001b[39m         timeout=\u001b[38;5;28mself\u001b[39m.step_timeout,\n\u001b[32m   (...)\u001b[39m\u001b[32m   2336\u001b[39m     ):\n\u001b[32m   2337\u001b[39m         \u001b[38;5;66;03m# emit output\u001b[39;00m\n\u001b[32m   2338\u001b[39m         \u001b[38;5;28;01myield from\u001b[39;00m output()\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\AI\\VSCodeProjects\\Persona-ChatAgent\\.venv\\Lib\\site-packages\\langgraph\\pregel\\loop.py:435\u001b[39m, in \u001b[36mPregelLoop.tick\u001b[39m\u001b[34m(self, input_keys)\u001b[39m\n\u001b[32m    425\u001b[39m     print_step_writes(\n\u001b[32m    426\u001b[39m         \u001b[38;5;28mself\u001b[39m.step,\n\u001b[32m    427\u001b[39m         writes,\n\u001b[32m   (...)\u001b[39m\u001b[32m    432\u001b[39m         ),\n\u001b[32m    433\u001b[39m     )\n\u001b[32m    434\u001b[39m \u001b[38;5;66;03m# all tasks have finished\u001b[39;00m\n\u001b[32m--> \u001b[39m\u001b[32m435\u001b[39m mv_writes, updated_channels = \u001b[43mapply_writes\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m    436\u001b[39m \u001b[43m    \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mcheckpoint\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    437\u001b[39m \u001b[43m    \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mchannels\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    438\u001b[39m \u001b[43m    \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mtasks\u001b[49m\u001b[43m.\u001b[49m\u001b[43mvalues\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    439\u001b[39m \u001b[43m    \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mcheckpointer_get_next_version\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    440\u001b[39m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    441\u001b[39m \u001b[38;5;66;03m# apply writes to managed values\u001b[39;00m\n\u001b[32m    442\u001b[39m \u001b[38;5;28;01mfor\u001b[39;00m key, values \u001b[38;5;129;01min\u001b[39;00m mv_writes.items():\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\AI\\VSCodeProjects\\Persona-ChatAgent\\.venv\\Lib\\site-packages\\langgraph\\pregel\\algo.py:317\u001b[39m, in \u001b[36mapply_writes\u001b[39m\u001b[34m(checkpoint, channels, tasks, get_next_version)\u001b[39m\n\u001b[32m    315\u001b[39m \u001b[38;5;28;01mfor\u001b[39;00m chan, vals \u001b[38;5;129;01min\u001b[39;00m pending_writes_by_channel.items():\n\u001b[32m    316\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m chan \u001b[38;5;129;01min\u001b[39;00m channels:\n\u001b[32m--> \u001b[39m\u001b[32m317\u001b[39m         \u001b[38;5;28;01mif\u001b[39;00m \u001b[43mchannels\u001b[49m\u001b[43m[\u001b[49m\u001b[43mchan\u001b[49m\u001b[43m]\u001b[49m\u001b[43m.\u001b[49m\u001b[43mupdate\u001b[49m\u001b[43m(\u001b[49m\u001b[43mvals\u001b[49m\u001b[43m)\u001b[49m \u001b[38;5;129;01mand\u001b[39;00m get_next_version \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[32m    318\u001b[39m             checkpoint[\u001b[33m\"\u001b[39m\u001b[33mchannel_versions\u001b[39m\u001b[33m\"\u001b[39m][chan] = get_next_version(\n\u001b[32m    319\u001b[39m                 max_version,\n\u001b[32m    320\u001b[39m                 channels[chan],\n\u001b[32m    321\u001b[39m             )\n\u001b[32m    322\u001b[39m         updated_channels.add(chan)\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\AI\\VSCodeProjects\\Persona-ChatAgent\\.venv\\Lib\\site-packages\\langgraph\\channels\\binop.py:89\u001b[39m, in \u001b[36mBinaryOperatorAggregate.update\u001b[39m\u001b[34m(self, values)\u001b[39m\n\u001b[32m     87\u001b[39m     values = values[\u001b[32m1\u001b[39m:]\n\u001b[32m     88\u001b[39m \u001b[38;5;28;01mfor\u001b[39;00m value \u001b[38;5;129;01min\u001b[39;00m values:\n\u001b[32m---> \u001b[39m\u001b[32m89\u001b[39m     \u001b[38;5;28mself\u001b[39m.value = \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43moperator\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mvalue\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mvalue\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m     90\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;01mTrue\u001b[39;00m\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\AI\\VSCodeProjects\\Persona-ChatAgent\\.venv\\Lib\\site-packages\\langgraph\\graph\\message.py:36\u001b[39m, in \u001b[36m_add_messages_wrapper.<locals>._add_messages\u001b[39m\u001b[34m(left, right, **kwargs)\u001b[39m\n\u001b[32m     32\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34m_add_messages\u001b[39m(\n\u001b[32m     33\u001b[39m     left: Optional[Messages] = \u001b[38;5;28;01mNone\u001b[39;00m, right: Optional[Messages] = \u001b[38;5;28;01mNone\u001b[39;00m, **kwargs: Any\n\u001b[32m     34\u001b[39m ) -> Union[Messages, Callable[[Messages, Messages], Messages]]:\n\u001b[32m     35\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m left \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;129;01mand\u001b[39;00m right \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[32m---> \u001b[39m\u001b[32m36\u001b[39m         \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mfunc\u001b[49m\u001b[43m(\u001b[49m\u001b[43mleft\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mright\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m     37\u001b[39m     \u001b[38;5;28;01melif\u001b[39;00m left \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;129;01mor\u001b[39;00m right \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[32m     38\u001b[39m         msg = (\n\u001b[32m     39\u001b[39m             \u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[33mMust specify non-null arguments for both \u001b[39m\u001b[33m'\u001b[39m\u001b[33mleft\u001b[39m\u001b[33m'\u001b[39m\u001b[33m and \u001b[39m\u001b[33m'\u001b[39m\u001b[33mright\u001b[39m\u001b[33m'\u001b[39m\u001b[33m. Only \u001b[39m\u001b[33m\"\u001b[39m\n\u001b[32m     40\u001b[39m             \u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[33mreceived: \u001b[39m\u001b[33m'\u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[33m'\u001b[39m\u001b[33mleft\u001b[39m\u001b[33m'\u001b[39m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mif\u001b[39;00m\u001b[38;5;250m \u001b[39mleft\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01melse\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[33m'\u001b[39m\u001b[33mright\u001b[39m\u001b[33m'\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m'\u001b[39m\u001b[33m.\u001b[39m\u001b[33m\"\u001b[39m\n\u001b[32m     41\u001b[39m         )\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\AI\\VSCodeProjects\\Persona-ChatAgent\\.venv\\Lib\\site-packages\\langgraph\\graph\\message.py:169\u001b[39m, in \u001b[36madd_messages\u001b[39m\u001b[34m(left, right, format)\u001b[39m\n\u001b[32m    165\u001b[39m     right = [right]  \u001b[38;5;66;03m# type: ignore[assignment]\u001b[39;00m\n\u001b[32m    166\u001b[39m \u001b[38;5;66;03m# coerce to message\u001b[39;00m\n\u001b[32m    167\u001b[39m left = [\n\u001b[32m    168\u001b[39m     message_chunk_to_message(cast(BaseMessageChunk, m))\n\u001b[32m--> \u001b[39m\u001b[32m169\u001b[39m     \u001b[38;5;28;01mfor\u001b[39;00m m \u001b[38;5;129;01min\u001b[39;00m \u001b[43mconvert_to_messages\u001b[49m\u001b[43m(\u001b[49m\u001b[43mleft\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    170\u001b[39m ]\n\u001b[32m    171\u001b[39m right = [\n\u001b[32m    172\u001b[39m     message_chunk_to_message(cast(BaseMessageChunk, m))\n\u001b[32m    173\u001b[39m     \u001b[38;5;28;01mfor\u001b[39;00m m \u001b[38;5;129;01min\u001b[39;00m convert_to_messages(right)\n\u001b[32m    174\u001b[39m ]\n\u001b[32m    175\u001b[39m \u001b[38;5;66;03m# assign missing ids\u001b[39;00m\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\AI\\VSCodeProjects\\Persona-ChatAgent\\.venv\\Lib\\site-packages\\langchain_core\\messages\\utils.py:365\u001b[39m, in \u001b[36mconvert_to_messages\u001b[39m\u001b[34m(messages)\u001b[39m\n\u001b[32m    363\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(messages, PromptValue):\n\u001b[32m    364\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m messages.to_messages()\n\u001b[32m--> \u001b[39m\u001b[32m365\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43m[\u001b[49m\u001b[43m_convert_to_message\u001b[49m\u001b[43m(\u001b[49m\u001b[43mm\u001b[49m\u001b[43m)\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mfor\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mm\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;129;43;01min\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mmessages\u001b[49m\u001b[43m]\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\AI\\VSCodeProjects\\Persona-ChatAgent\\.venv\\Lib\\site-packages\\langchain_core\\messages\\utils.py:365\u001b[39m, in \u001b[36m<listcomp>\u001b[39m\u001b[34m(.0)\u001b[39m\n\u001b[32m    363\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(messages, PromptValue):\n\u001b[32m    364\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m messages.to_messages()\n\u001b[32m--> \u001b[39m\u001b[32m365\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m [\u001b[43m_convert_to_message\u001b[49m\u001b[43m(\u001b[49m\u001b[43mm\u001b[49m\u001b[43m)\u001b[49m \u001b[38;5;28;01mfor\u001b[39;00m m \u001b[38;5;129;01min\u001b[39;00m messages]\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\AI\\VSCodeProjects\\Persona-ChatAgent\\.venv\\Lib\\site-packages\\langchain_core\\messages\\utils.py:344\u001b[39m, in \u001b[36m_convert_to_message\u001b[39m\u001b[34m(message)\u001b[39m\n\u001b[32m    342\u001b[39m     msg = \u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[33mUnsupported message type: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;28mtype\u001b[39m(message)\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m\"\u001b[39m\n\u001b[32m    343\u001b[39m     msg = create_message(message=msg, error_code=ErrorCode.MESSAGE_COERCION_FAILURE)\n\u001b[32m--> \u001b[39m\u001b[32m344\u001b[39m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mNotImplementedError\u001b[39;00m(msg)\n\u001b[32m    346\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m _message\n",
      "\u001b[31mNotImplementedError\u001b[39m: Unsupported message type: <class 'list'>\nFor troubleshooting, visit: https://python.langchain.com/docs/troubleshooting/errors/MESSAGE_COERCION_FAILURE "
     ]
    }
   ],
   "source": [
    "config = {\"configurable\": {\"thread_id\": \"5\"}}\n",
    "while True:\n",
    "    user_input = input(\"User: \")\n",
    "    print(\"User:\", user_input)\n",
    "    if user_input.lower() in [\"quit\", \"exit\", \"q\"]:\n",
    "        print(\"Goodbye!\")\n",
    "        break\n",
    "    for event in app.stream({\"messages\": [HumanMessage(content=user_input)]}, config):\n",
    "        for value in event.values():\n",
    "            print(\"Assistant:\", value[\"messages\"][-1].content)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
