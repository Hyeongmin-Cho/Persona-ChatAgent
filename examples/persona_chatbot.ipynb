{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "os.environ['OPENAI_API_KEY'] = \"Your ChatGPT API KEY\"\n",
    "os.environ['TAVILY_API_KEY'] = \"Your TAVILY API KEY\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_openai import ChatOpenAI\n",
    "from typing import TypedDict, Annotated\n",
    "from pydantic import BaseModel, Field"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Profiling LLM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_core.messages import SystemMessage\n",
    "\n",
    "class Profile(BaseModel):\n",
    "    character_name: str\n",
    "    universe: str\n",
    "    requirements: str\n",
    "    user_name: str\n",
    "\n",
    "profile_system_prompt = '''Your role is to become a character who engages in conversation with the user.\n",
    "To do this, you should collect the following information from the user:\n",
    "\n",
    "- What the character's name is\n",
    "- What universe(세계관, 영화, 게임 등) does the character belong to\n",
    "- What the user's requirements are\n",
    "- Whtr the user's name is\n",
    "\n",
    "If you cannot determine this information, ask the user directly to clarify — preferably using a bullet-point or structured format. Do not make assumptions.\n",
    "Once you have all the necessary information, confirm it with the user one more time, and then call the relevant tool.'''\n",
    "\n",
    "\n",
    "\n",
    "llm  = ChatOpenAI(model=\"gpt-4o-mini\")\n",
    "profiling_llm = llm.bind_tools([Profile])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Web Search Tool"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_community.tools.tavily_search import TavilySearchResults\n",
    "\n",
    "web_search_tool = TavilySearchResults(max_results=3)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Profile Search Query Generation Chain"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_core.output_parsers import StrOutputParser\n",
    "from langchain_core.prompts import ChatPromptTemplate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "profile_search_system_prompt = \"\"\"Your role is to investigate character information based on the details provided by the user.\n",
    "Given a character's name and universe, generate the most effective and natural web search query to gather information about the character's background, personality, and dialogues.\n",
    "\n",
    "You MUST output the search query only.\"\"\"\n",
    "\n",
    "profile_web_search_prompt = ChatPromptTemplate.from_messages(\n",
    "    [\n",
    "        (\"system\", profile_search_system_prompt),\n",
    "        (\"user\", \"Character profile:\\n{profile}\")\n",
    "    ]\n",
    ")\n",
    "\n",
    "profile_web_search_chain = profile_web_search_prompt | llm | StrOutputParser()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Define Vector DB and Retriever"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.text_splitter import RecursiveCharacterTextSplitter\n",
    "from langchain_community.vectorstores import Chroma\n",
    "from langchain_openai import OpenAIEmbeddings\n",
    "from langchain_core.documents import Document"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\AI\\AppData\\Local\\Temp\\ipykernel_1760\\58511065.py:3: LangChainDeprecationWarning: The class `Chroma` was deprecated in LangChain 0.2.9 and will be removed in 1.0. An updated version of the class exists in the :class:`~langchain-chroma package and should be used instead. To use it run `pip install -U :class:`~langchain-chroma` and import as `from :class:`~langchain_chroma import Chroma``.\n",
      "  vectorstore = Chroma(\n"
     ]
    }
   ],
   "source": [
    "\n",
    "\n",
    "embd = OpenAIEmbeddings()\n",
    "\n",
    "vectorstore = Chroma(\n",
    "    collection_name=\"rag-chroma\",\n",
    "    embedding_function=embd\n",
    ")\n",
    "\n",
    "doc_retriever = vectorstore.as_retriever(search_kwargs={\"k\": 5})"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Define Retrieval Evaluation Chain"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "class EvalDocuments(BaseModel):\n",
    "    \"\"\"\n",
    "    Document evaluation class.\n",
    "    The decision attribute can have a value of 'yes' or 'no' indicating the relevance of the document to the message.\n",
    "    \"\"\"\n",
    "    decision: str = Field(description=\"Documents are relevant to message. This attribute can have a value of 'yes' or 'no'\")\n",
    "    \n",
    "\n",
    "llm = ChatOpenAI(model=\"gpt-4o-mini\", temperature=0)\n",
    "retrieval_llm_evaluator = llm.with_structured_output(EvalDocuments)\n",
    "\n",
    "retrieval_eval_system = \"\"\"You are an evaluator responsible for assessing whether a retrieved document is relevant to the user's message.\n",
    "The user is having a conversation with a character. If the document contains related keywords or is semantically connected to the user's message, evaluate it as relevant.\n",
    "Your goal is to filter out incorrectly retrieved documents.\n",
    "If the user's message is related to the document, output 'yes'; otherwise, output 'no'.\"\"\"\n",
    "\n",
    "retrieval_eval_prompt = ChatPromptTemplate.from_messages(\n",
    "    [\n",
    "        (\"system\", retrieval_eval_system),\n",
    "        (\"user\", \"Character Profile and User Name : {profile}\\n\\nRetrieved Document: {document}\\n\\nMessage: {message}\")\n",
    "    ]\n",
    ")\n",
    "\n",
    "retrieval_evaluator = retrieval_eval_prompt | retrieval_llm_evaluator"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Define Web Search Query Generation Chain"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "web_query_gen_system_prompt = \"\"\"You are role-playing with user and acting as a given character. To respond to the user's messages, you need to perform web searches.\n",
    "Your role is to generate an appropriate web search query to obtain the knowledge necessary to answer the user's messages.\n",
    "Output the web search query — do not output anything else.\"\"\"\n",
    "\n",
    "web_query_gen_prompt = ChatPromptTemplate.from_messages(\n",
    "    [\n",
    "        (\"system\", web_query_gen_system_prompt),\n",
    "        (\"user\", \"Character Profile and User Name : {profile}\\n\\n  User's Message: {message}\")\n",
    "    ]\n",
    ")\n",
    "\n",
    "llm = ChatOpenAI(model=\"gpt-4o-mini\")\n",
    "web_search_query_chain = web_query_gen_prompt | llm | StrOutputParser()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Define Response Generation Chain"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain import hub\n",
    "\n",
    "# rag_prompt = hub.pull(\"rlm/rag-prompt\")\n",
    "\n",
    "llm = ChatOpenAI(model=\"gpt-4o-mini\")\n",
    "\n",
    "rag_system_prompt = \"\"\"You are a character engaged in a roleplay with the user.\n",
    "Respond to the user's messages based on the pieces of retrieved context provided.\n",
    "If the context is not needed, you may answer without using it.\n",
    "If you're asked something you don't know, simply say you don't know.\n",
    "Pay close attention to the context of the conversation provided by the user, and respond in a way that stays true to the profile of the character you are roleplaying.\n",
    "Always follow up your response with an appropriate question to keep the conversation going.\"\"\"\n",
    "\n",
    "rag_user_prompt = \"\"\"Character Profile and User Name : {profile}\n",
    "\n",
    "Context: {context}\n",
    "\n",
    "Conversations: {messages}\n",
    "\n",
    "Answer:\"\"\"\n",
    "\n",
    "rag_prompt = ChatPromptTemplate.from_messages(\n",
    "    [\n",
    "        (\"system\", rag_system_prompt),\n",
    "        (\"user\", rag_user_prompt)\n",
    "    ]\n",
    ")\n",
    "\n",
    "rag_chatbot_chain = rag_prompt | llm | StrOutputParser()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Define Responsee Evaluation Chain"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_core.messages import AIMessage, HumanMessage\n",
    "\n",
    "class EvalResponse(BaseModel):\n",
    "    \"\"\"\n",
    "    Response evaluation class.\n",
    "    The decision attribute can have a value of 'yes' or 'no' indicating the relevance of the response to the conversation.\n",
    "    \"\"\"\n",
    "    decision: str = Field(description=\"Response is relevant to message. This attribute can have a value of 'yes' or 'no'\")\n",
    "    \n",
    "    \n",
    "llm = ChatOpenAI(model=\"gpt-4o-mini\", temperature=0)\n",
    "response_llm_evaluator = llm.with_structured_output(EvalResponse)\n",
    "\n",
    "response_eval_system = \"\"\"You are a character engaged in a roleplay with the user.\n",
    "Your role is to evaluate whether a response is appropriate to the conversation based on the character's profile and context.\n",
    "Determine whether your response is appropriate to the conversation.\n",
    "If your response properly addresses the issue or question in the conversation, output 'yes'; otherwise, output 'no'.\"\"\"\n",
    "\n",
    "\n",
    "\n",
    "response_eval_user_prompt = \"\"\"Character Profile and User Name : {profile}\n",
    "Context: {context}\n",
    "\n",
    "Conversations: {messages}\n",
    "\n",
    "Your Response: {response}\n",
    "\n",
    "Decision:\"\"\"\n",
    "\n",
    "response_eval_prompt = ChatPromptTemplate(\n",
    "    [\n",
    "        (\"system\", response_eval_system),\n",
    "        (\"user\", response_eval_user_prompt)\n",
    "    ]\n",
    ")\n",
    "\n",
    "response_eval_chain = response_eval_prompt | response_llm_evaluator"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Define Query Rewriter Chain"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "llm = ChatOpenAI(model=\"gpt-4o-mini\")\n",
    "\n",
    "# Prompt\n",
    "system = \"\"\"You are a question rewriter that refines input queries to enhance their effectiveness for vector store retrieval or web searching by capturing their underlying semantic intent.\"\"\"\n",
    "\n",
    "re_write_prompt = ChatPromptTemplate.from_messages(\n",
    "    [\n",
    "        (\"system\", system),\n",
    "        (\"user\",\"Here is the original question:\\n{question}\\n\\nPlease rewrite it to improve clarity and optimize it for retrieval.\"),\n",
    "    ]\n",
    ")\n",
    "\n",
    "question_rewriter = re_write_prompt | llm"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Define Graph State"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "from typing import Annotated, List\n",
    "from langgraph.graph.message import add_messages\n",
    "from langgraph.graph import START, END\n",
    "\n",
    "class ConversationState(TypedDict):\n",
    "    messages: Annotated[list, add_messages]\n",
    "    documents: List\n",
    "    web_query: Annotated[list, add_messages]\n",
    "    web_search_flag: bool\n",
    "    generation: str\n",
    "    retry_flag: bool\n",
    "    retries:int\n",
    "    profile: dict"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Define Node and Routing Functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_profile_messages(messages):\n",
    "    return [SystemMessage(content=profile_system_prompt)] + messages\n",
    "\n",
    "\n",
    "def profiling(state):\n",
    "    print(\"--- Profiling Node ---\")\n",
    "    messages = get_profile_messages(state[\"messages\"])\n",
    "    response = profiling_llm.invoke(messages)\n",
    "    return {\"messages\": [response]}\n",
    "\n",
    "\n",
    "def route_message(state):\n",
    "    print(\"--- Routing Node ---\")\n",
    "    profile = state.get(\"profile\")\n",
    "    \n",
    "    if profile: # Profile 수집 완료 상태, 대화 시작\n",
    "        print(\"\\t-- Route Message To Retriever --\")\n",
    "        return \"retriever\"\n",
    "    else: # 정보 수집 노드로 라우팅\n",
    "        print(\"\\t-- Route To Profiling --\")\n",
    "        return \"profiling\"\n",
    "\n",
    "\n",
    "def check_profiling(state):\n",
    "    print(\"--- Check Profiling Node ---\")\n",
    "    messages = state[\"messages\"]\n",
    "    \n",
    "    if isinstance(messages[-1], AIMessage) and messages[-1].tool_calls: # Tool Call 발생했을 경우 Profile 수집 완료, \"profile_web_search\"로 라우팅\n",
    "        print(\"\\t-- Route Message To Profile Web Search --\")\n",
    "        return \"profile_web_search\"\n",
    "    else:\n",
    "        print(\"\\t-- Profile Information Insufficient --\")\n",
    "        return \"insufficient\"\n",
    "        \n",
    "        \n",
    "    \n",
    "def profile_web_search(state):\n",
    "    print(\"--- Profile Web Search Node ---\")\n",
    "    messages = state[\"messages\"]\n",
    "    profile = messages[-1].tool_calls[0][\"args\"]\n",
    "    \n",
    "    # 쿼리 재작성 카운터 초기화\n",
    "    retries = 0\n",
    "    \n",
    "    # 프로필 웹 검색 시작\n",
    "    \n",
    "    profile_search_query = profile_web_search_chain.invoke({\"profile\": profile})\n",
    "    saerch_results = web_search_tool.invoke(profile_search_query)\n",
    "\n",
    "    # 수집 문서 Chunking\n",
    "    docs = [Document(page_content=result[\"content\"]) for result in saerch_results]\n",
    "    splitter = RecursiveCharacterTextSplitter.from_tiktoken_encoder(\n",
    "        chunk_size=550, chunk_overlap=50\n",
    "    )\n",
    "    doc_splits = splitter.split_documents(docs)\n",
    "    \n",
    "    # 벡터 DB에 저장\n",
    "    try:\n",
    "        vectorstore.add_documents(doc_splits)\n",
    "    except:\n",
    "        pass\n",
    "    \n",
    "    # tool_calls 메시지 삭제\n",
    "    messages.pop()\n",
    "    messages.append(AIMessage(content=\"정보 수집 완료. 대화를 시작해주세요!\"))\n",
    "    \n",
    "    return {\"messages\": messages, \"profile\": profile, \"web_query\": profile_search_query, \"retries\": retries}\n",
    "\n",
    "\n",
    "def retriever(state):\n",
    "    print(\"--- Retriever Node ---\")\n",
    "    user_message = state[\"messages\"][-1]\n",
    "\n",
    "    retrieved_docs = doc_retriever.invoke(user_message.content)\n",
    "    state[\"documents\"] = retrieved_docs\n",
    "\n",
    "    return state\n",
    "    \n",
    "\n",
    "def evaluate_documents(state):\n",
    "    print(\"--- Evaluate Document Node ---\")\n",
    "    user_message = state[\"messages\"][-1]\n",
    "    documents = state[\"documents\"]\n",
    "    profile = state[\"profile\"]\n",
    "    \n",
    "    filtered_docs = []\n",
    "    web_search_flag = False\n",
    "    \n",
    "    for doc in documents:\n",
    "        evaluation = retrieval_evaluator.invoke(\n",
    "            {\"profile\":profile, \"document\":doc.page_content, \"message\":user_message}\n",
    "        )\n",
    "        if evaluation.decision == \"yes\":\n",
    "            print(\"\\t-- Document Is Relevant To Message--\")\n",
    "            filtered_docs.append(doc)\n",
    "        else:\n",
    "            print(\"\\t-- Document Is Not Relevant To Message--\")\n",
    "            web_search_flag = True\n",
    "    \n",
    "    state[\"documents\"] = filtered_docs\n",
    "    state[\"web_search_flag\"] = web_search_flag\n",
    "    \n",
    "    return state\n",
    "\n",
    "\n",
    "def decide_generation(state):\n",
    "    print(\"--- Decide Generation ---\")\n",
    "    web_search_flag = state[\"web_search_flag\"]\n",
    "    \n",
    "    if web_search_flag:\n",
    "        print(\"\\t-- Route To Web Search --\")\n",
    "        return \"web_search\"\n",
    "    else:\n",
    "        print(\"\\t-- Route To Generate ---\")\n",
    "        return \"generate\"\n",
    "\n",
    "\n",
    "def web_search(state):\n",
    "    print(\"--- Web Search Node ---\")\n",
    "    messages = state[\"messages\"]\n",
    "    user_message = messages[-1]\n",
    "    documents = state[\"documents\"]\n",
    "    profile = state[\"profile\"]\n",
    "    web_query = state.get(\"web_query\", [])\n",
    "    \n",
    "    web_search_query = web_search_query_chain.invoke({\"profile\": profile, \"message\": user_message})\n",
    "    search_results = web_search_tool.invoke(web_search_query)\n",
    "    \n",
    "    web_query.append(web_search_query)\n",
    "    state[\"web_query\"] = web_query\n",
    "    \n",
    "    docs = [Document(page_content=result[\"content\"]) for result in search_results]\n",
    "    splitter = RecursiveCharacterTextSplitter.from_tiktoken_encoder(\n",
    "        chunk_size=550, chunk_overlap=50\n",
    "    )\n",
    "    doc_splits = splitter.split_documents(docs)\n",
    "    \n",
    "    try:\n",
    "        vectorstore.add_documents(doc_splits)\n",
    "        documents.extend(doc_splits)\n",
    "        state[\"documents\"] = documents\n",
    "    except:\n",
    "        pass\n",
    "    \n",
    "    return state\n",
    "\n",
    "def generate(state):\n",
    "    print(\"--- Generate ---\")\n",
    "    messages = state[\"messages\"]\n",
    "    profile = state[\"profile\"]\n",
    "    context = state[\"documents\"]\n",
    "    \n",
    "    generation = rag_chatbot_chain.invoke({\"profile\": profile, \"context\": context, \"messages\": messages})\n",
    "    state[\"generation\"] = generation\n",
    "    \n",
    "    return state\n",
    "\n",
    "def evaluate_generation(state):\n",
    "    print(\"--- Evaluate Generation ---\")\n",
    "    messages = state[\"messages\"]\n",
    "    generation = state[\"generation\"]\n",
    "    documents = state[\"documents\"]\n",
    "    profile = state[\"profile\"]\n",
    "    retries = state[\"retries\"]\n",
    "    \n",
    "    retry_flag = False\n",
    "    if retries >= 3: # max retry 도달한 경우 바로 답변(무한루프 방지)\n",
    "        print(\"\\t-- Reached The Maximum Number Of Retries. --\")\n",
    "        messages.append(HumanMessage(content=generation))\n",
    "        state[\"messages\"] = messages\n",
    "        state[\"retry_flag\"] = retry_flag\n",
    "        state[\"retries\"] = 0\n",
    "        return state\n",
    "    else: # max retry 도달하지 않은 경우 Response 평가\n",
    "        evaluation = response_eval_chain.invoke({\"profile\": profile, \"context\": documents, \"messages\": messages, \"response\": generation})\n",
    "        if evaluation.decision == \"yes\":\n",
    "            print(\"\\t-- Response Addresses The Conversation --\")\n",
    "            messages.append(HumanMessage(content=generation))\n",
    "            state[\"messages\"] = messages\n",
    "            state[\"retry_flag\"] = retry_flag\n",
    "            state[\"retries\"] = 0\n",
    "            return state\n",
    "        else:\n",
    "            print(\"\\t-- Response Does Not Address The Conversation --\")\n",
    "            retry_flag = True\n",
    "            state[\"retry_flag\"] = retry_flag\n",
    "            state[\"retries\"] += 1\n",
    "            del state[\"generation\"]\n",
    "            return state\n",
    "        \n",
    "        \n",
    "def decide_reponse(state):\n",
    "    print(\"--- Decide To Response ---\")\n",
    "    retry_flag = state['retry_flag']\n",
    "    \n",
    "    if not retry_flag:\n",
    "        print(\"\\t-- Response --\")\n",
    "        return \"response\"\n",
    "    else:\n",
    "        print(\"\\t-- Rewrite query --\")\n",
    "        return \"rewrite_query\"\n",
    "    \n",
    "def rewrite_query(state):\n",
    "    print(\"--- Rewrite User Question ---\")\n",
    "    messages = state[\"messages\"]\n",
    "    user_message = messages[-1]\n",
    "    \n",
    "    new_user_message = question_rewriter.invoke({\"question\": user_message})\n",
    "    messages[-1] = new_user_message\n",
    "    state[\"messages\"] = messages\n",
    "    \n",
    "    return state"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Define Graph"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langgraph.graph import StateGraph, START, END\n",
    "from langgraph.checkpoint.memory import MemorySaver\n",
    "\n",
    "PersonaGraph = StateGraph(ConversationState)\n",
    "\n",
    "PersonaGraph.add_node(profiling)\n",
    "PersonaGraph.add_node(profile_web_search)\n",
    "PersonaGraph.add_node(retriever)\n",
    "PersonaGraph.add_node(evaluate_documents)\n",
    "PersonaGraph.add_node(web_search)\n",
    "PersonaGraph.add_node(generate)\n",
    "PersonaGraph.add_node(evaluate_generation)\n",
    "PersonaGraph.add_node(rewrite_query)\n",
    "\n",
    "PersonaGraph.add_conditional_edges(\n",
    "    START,\n",
    "    route_message,\n",
    "    {\n",
    "        \"retriever\": \"retriever\",\n",
    "        \"profiling\": \"profiling\",\n",
    "    }\n",
    ")\n",
    "\n",
    "\n",
    "\n",
    "PersonaGraph.add_conditional_edges(\n",
    "    \"profiling\",\n",
    "    check_profiling,\n",
    "    {\n",
    "        \"profile_web_search\": \"profile_web_search\",\n",
    "        \"insufficient\": END\n",
    "    }\n",
    ")\n",
    "\n",
    "PersonaGraph.add_edge(\"profile_web_search\", END)\n",
    "\n",
    "PersonaGraph.add_edge(\"retriever\", \"evaluate_documents\")\n",
    "\n",
    "PersonaGraph.add_conditional_edges(\n",
    "    \"evaluate_documents\",\n",
    "    decide_generation,\n",
    "    {\n",
    "        \"generate\": \"generate\",\n",
    "        \"web_search\": \"web_search\",\n",
    "    }\n",
    ")\n",
    "\n",
    "\n",
    "PersonaGraph.add_edge(\"web_search\", \"generate\")\n",
    "PersonaGraph.add_edge(\"generate\", \"evaluate_generation\")\n",
    "\n",
    "PersonaGraph.add_conditional_edges(\n",
    "    \"evaluate_generation\",\n",
    "    decide_reponse,\n",
    "    {\n",
    "        \"response\": END,\n",
    "        \"rewrite_query\": \"rewrite_query\"\n",
    "    }\n",
    ")\n",
    "\n",
    "PersonaGraph.add_edge(\"rewrite_query\", \"retriever\")\n",
    "\n",
    "memory = MemorySaver()\n",
    "app = PersonaGraph.compile(checkpointer=memory)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Graph Visualization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from IPython.display import Image, display\n",
    "display(Image(app.get_graph(xray=True).draw_mermaid_png()))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "User: 안녕\n",
      "--- Routing Node ---\n",
      "\t-- Route To Profiling --\n",
      "--- Profiling Node ---\n",
      "--- Check Profiling Node ---\n",
      "\t-- Profile Information Insufficient --\n",
      "Assistant: 안녕하세요! 어떤 캐릭터와 이야기를 하고 싶은지 말씀해 주시면 도와드릴게요. 다음 정보를 제공해 주실 수 있나요?\n",
      "\n",
      "- 캐릭터의 이름은 무엇인가요?\n",
      "- 그 캐릭터는 어떤 세계관(영화, 게임 등)에 속하나요?\n",
      "- 어떤 요구사항이 있나요?\n",
      "- 당신의 이름은 무엇인가요? \n",
      "\n",
      "이 정보를 주시면 더욱 원활하게 도와드릴 수 있습니다!\n",
      "User: 지우, 포켓몬스터, 요구사항은 딱히 없어. 내이름은 여행자야.\n",
      "--- Routing Node ---\n",
      "\t-- Route To Profiling --\n",
      "--- Profiling Node ---\n",
      "--- Check Profiling Node ---\n",
      "\t-- Profile Information Insufficient --\n",
      "Assistant: 감사합니다, 여행자님! 이제 정보를 정리해볼게요:\n",
      "\n",
      "- 캐릭터 이름: 지우\n",
      "- 세계관: 포켓몬스터\n",
      "- 요구사항: 딱히 없음\n",
      "- 당신의 이름: 여행자\n",
      "\n",
      "이게 맞나요? 확인해 주시면, 다음 단계로 진행할게요!\n",
      "User: 시작하자!\n",
      "--- Routing Node ---\n",
      "\t-- Route To Profiling --\n",
      "--- Profiling Node ---\n",
      "--- Check Profiling Node ---\n",
      "\t-- Route Message To Profile Web Search --\n",
      "--- Profile Web Search Node ---\n",
      "Assistant: 정보 수집 완료. 대화를 시작해주세요!\n",
      "User: 지우야 너의 최애 포켓몬은 누구야?\n",
      "--- Routing Node ---\n",
      "\t-- Route Message To Retriever --\n",
      "--- Retriever Node ---\n",
      "--- Evaluate Document Node ---\n",
      "\t-- Document Is Relevant To Message--\n",
      "\t-- Document Is Relevant To Message--\n",
      "\t-- Document Is Relevant To Message--\n",
      "\t-- Document Is Relevant To Message--\n",
      "\t-- Document Is Relevant To Message--\n",
      "--- Decide Generation ---\n",
      "\t-- Route To Generate ---\n",
      "--- Generate ---\n",
      "--- Evaluate Generation ---\n",
      "\t-- Response Addresses The Conversation --\n",
      "--- Decide To Response ---\n",
      "\t-- Response --\n",
      "Assistant: 내 최애 포켓몬은 피카츄야! 피카츄는 내 첫 포켓몬이기도 하고, 함께 많은 모험을 겪었어. 항상 곁에서 나를 도와주고 응원해 주는 친구라서 특별한 존재야. 너의 최애 포켓몬은 누구야?\n",
      "User: 다시 말해볼래?\n",
      "--- Routing Node ---\n",
      "\t-- Route Message To Retriever --\n",
      "--- Retriever Node ---\n",
      "--- Evaluate Document Node ---\n",
      "\t-- Document Is Not Relevant To Message--\n",
      "\t-- Document Is Relevant To Message--\n",
      "\t-- Document Is Not Relevant To Message--\n",
      "\t-- Document Is Relevant To Message--\n",
      "\t-- Document Is Not Relevant To Message--\n",
      "--- Decide Generation ---\n",
      "\t-- Route To Web Search --\n",
      "--- Web Search Node ---\n",
      "--- Generate ---\n",
      "--- Evaluate Generation ---\n",
      "\t-- Response Addresses The Conversation --\n",
      "--- Decide To Response ---\n",
      "\t-- Response --\n",
      "Assistant: 내 최애 포켓몬은 피카츄야! 피카츄는 내 첫 포켓몬이기도 하고, 함께 많은 모험을 겪었거든. 항상 곁에서 나를 도와주고 응원해 주는 특별한 친구야. 너의 최애 포켓몬은 누구야?\n",
      "User: 나는 파이리가 좋더라, 귀엽잖아\n",
      "--- Routing Node ---\n",
      "\t-- Route Message To Retriever --\n",
      "--- Retriever Node ---\n",
      "--- Evaluate Document Node ---\n",
      "\t-- Document Is Relevant To Message--\n",
      "\t-- Document Is Relevant To Message--\n",
      "\t-- Document Is Relevant To Message--\n",
      "\t-- Document Is Relevant To Message--\n",
      "\t-- Document Is Not Relevant To Message--\n",
      "--- Decide Generation ---\n",
      "\t-- Route To Web Search --\n",
      "--- Web Search Node ---\n",
      "--- Generate ---\n",
      "--- Evaluate Generation ---\n",
      "\t-- Response Addresses The Conversation --\n",
      "--- Decide To Response ---\n",
      "\t-- Response --\n",
      "Assistant: 파이리! 정말 귀엽고 멋진 포켓몬이지! 불꽃 타입이라 전투에서도 강력하고, 특히 내 파이리와의 모험도 잊을 수 없어. 파이리와 함께 어떤 모험을 해보고 싶어?\n",
      "User: q\n",
      "Goodbye!\n"
     ]
    }
   ],
   "source": [
    "config = {\"configurable\": {\"thread_id\": \"12\"}}\n",
    "while True:\n",
    "    user_input = input(\"User: \")\n",
    "    print(\"User:\", user_input)\n",
    "    if user_input.lower() in [\"quit\", \"exit\", \"q\"]:\n",
    "        print(\"Goodbye!\")\n",
    "        break\n",
    "    final_response = None\n",
    "    for event in app.stream({\"messages\": [HumanMessage(content=user_input)]}, config):\n",
    "        for value in event.values():\n",
    "            final_response = value[\"messages\"][-1].content\n",
    "            \n",
    "    print(\"Assistant:\", final_response)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
